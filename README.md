# var-imp-sim
To accompany the article "Explainable AI Methods to Establish Predictor Importance: Methods and Challenges"
Rosa Lavelle-Hill (University of Copenhagen), Gavin Smith (University of Nottingham), and Kou Murayama (University of TÃ¼bingen)

Abstract
With more researchers in psychology using machine learning methods to model large datasets, many are also looking to eXplainable AI (XAI) methods to understand how their model works and to gain insights into the most important predictors. However, the methodological approach for accurately quantifying predictor importance in machine learning is not as straightforward or as well-established as in psychology. Not only are there a large number of potential tools, but there are also unresolved challenges when using XAI for psychological research. This tutorial first provides an overview of the breadth of XAI approaches most useful to psychologists, categorizing them along two dimensions: model-specific vs. model-agnostic and producing local vs. global explanations. Importantly, we then highlight and discuss some of the challenges that psychologists can encounter when using XAI metrics to understand predictor importance, namely, how to attribute importance when there exists multicollinearity, when there are complex (non-linear) interactions, and/or multiple possible solutions to the prediction problem. This tutorial is accompanied by a Python code repository where readers can interact with the code to understand better how changing characteristics of the data or key parameters in the modeling process can influence different XAI outputs. Finally, we provide some recommendations for good and open practices for psychologists using XAI methods to inform theory moving forward.

[XAI_Fig.pdf](https://github.com/Rosa-Lavelle-Hill/var-imp-sim/files/13587126/XAI_Fig.pdf)
